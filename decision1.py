#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Sat Nov  9 06:50:58 2024@author: frankbogle"""import matplotlib.pyplot as pltimport pandas as pdfrom sklearn.tree import DecisionTreeClassifier, plot_treefrom sklearn.metrics import confusion_matrixfrom sklearn.metrics import accuracy_score, classification_reportfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import GridSearchCVimport seaborn as snsdf = pd.read_csv("/Users/frankbogle/Downloads/framingham.csv")print("Framingham shape with na values: ", df.shape)# check levels of na dataprint(df.isna().sum().sort_values(ascending=False))# Logistic regression testing example so drop na valuesdf = df.dropna()print("Framingham shape without na values: ", df.shape)print(df.head())# Create dataframes for random forest classificationX = df.drop(['TenYearCHD'], axis = 1)print(X.shape)print(X.head())y = df['TenYearCHD']print(y.shape)print(y.head())# Create training and test datasetsx_train ,x_test ,y_train ,y_test = train_test_split(X,y,test_size=0.2)def train_best_decision_tree(x_train, y_train, x_test, y_test, param_grid = None, cv =  5, scoring = 'accuracy'):    # Default parameter grid if none is provided    if param_grid is None:        param_grid = {            'max_depth': [None, 3, 5, 7, 10],            'min_samples_split': [5, 10, 20],            'min_samples_leaf': [5, 10, 15]        }    # Initialize and fit GridSearchCV    grid_search = GridSearchCV(        estimator=DecisionTreeClassifier(random_state=42),        param_grid=param_grid,        cv=cv,        scoring=scoring,        n_jobs=-1    )    grid_search.fit(x_train, y_train)        # Retrieve best model and make predictions    best_model = grid_search.best_estimator_    y_pred = best_model.predict(x_test)        # Evaluation metrics    accuracy = accuracy_score(y_test, y_pred)    best_params = grid_search.best_params_        return best_model, accuracy, best_paramsbest_model, accuracy, best_params = train_best_decision_tree(x_train, y_train, x_test, y_test)print("Best Model Accuracy:", accuracy)print("Best Model Parameters:", best_params)# Evaluate best modely_pred_best = best_model.predict(x_test)# Create confusion matrix for plottingconfusion_m = confusion_matrix(y_test, y_pred_best, labels = [0, 1])# Classification reportreport = classification_report(y_test, y_pred_best)print("Classification Report:\n", report)# Use Seaborn heatmap to plotplt.figure(figsize=(8, 6))sns.heatmap(confusion_m, annot = True, fmt = "d", cmap = "Blues", cbar = False)plt.xlabel("Predicted Labels")plt.ylabel("True Labels")plt.title("Random Forest Confusion Matrix")plt.show()# Plot decision treeplt.figure(figsize=(15, 10))plot_tree(best_model, feature_names=X.columns, class_names=["Class 0", "Class 1"], filled = True)plt.show()